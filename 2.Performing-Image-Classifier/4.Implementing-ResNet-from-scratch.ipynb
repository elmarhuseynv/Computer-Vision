{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_module(data,\n",
    "                     filters,\n",
    "                     strides,\n",
    "                     reduce=False,\n",
    "                     reg=0.0001,\n",
    "                     bn_eps=2e-5,\n",
    "                     bn_momentum=0.9):\n",
    "    bn_1 = BatchNormalization(axis=-1, \n",
    "                              epsilon=bn_eps, \n",
    "                              momentum=bn_momentum)(data)\n",
    "    act_1 = ReLU()(bn_1)\n",
    "    conv_1 = Conv2D(filters=int(filters / 4.),\n",
    "                    kernel_size=(1,1),\n",
    "                    use_bias=False,\n",
    "                    kernel_regularizer=l2(reg))(act_1)\n",
    "    bn_2 = BatchNormalization(axis=-1, \n",
    "                              epsilon=bn_eps, \n",
    "                              momentum=bn_momentum)(conv_1)\n",
    "    act_2 = ReLU()(bn_2)\n",
    "    conv_2 = Conv2D(filters=int(filters / 4.),\n",
    "                    kernel_size=(3,3),\n",
    "                    strides=stride,\n",
    "                    padding='same',\n",
    "                    use_bias=False,\n",
    "                    kernel_regularizer=l2(reg))(act_2)\n",
    "    bn_3 = BatchNormalization(axis=-1,\n",
    "                              epsilon=bn_eps,\n",
    "                              momentum=bn_momentum)(conv_2)\n",
    "    act_3 = ReLU()(bn_3)\n",
    "    conv_3 = Conv2D(filters=filters,\n",
    "                    kernel_size=(1,1),\n",
    "                    use_bias=False,\n",
    "                    kernel_regularizer=l2(reg))(act_3)\n",
    "    if reduce:\n",
    "        shortcut = Conv2D(filters=filters,\n",
    "                          kernel_size=(1,1),\n",
    "                          strides=stride,\n",
    "                          use_bias=False,\n",
    "                          kernel_regularizer=l2(reg))(act_3)\n",
    "    x = Add()([conv_3, shortcut])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape,\n",
    "                 classes,\n",
    "                 stages,\n",
    "                 filters,\n",
    "                 reg=1e-3,\n",
    "                 bn_eps=2e-5,\n",
    "                 bn_momentum=0.9):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = BatchNormalization(axis=-1,\n",
    "                           epsilon=bn_eps,\n",
    "                           momentum=bn_momentum)(inputs)\n",
    "\n",
    "    x = Conv2D(filters[0], (3, 3),\n",
    "               use_bias=False,\n",
    "               padding='same',\n",
    "               kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "    for i in range(len(stages)):\n",
    "        # Initialize the stride, then apply a residual module\n",
    "        # used to reduce the spatial size of the input volume.\n",
    "        stride = (1, 1) if i == 0 else (2, 2)\n",
    "        x = residual_module(data=x,\n",
    "                            filters=filters[i + 1],\n",
    "                            stride=stride,\n",
    "                            reduce=True,\n",
    "                            bn_eps=bn_eps,\n",
    "                            bn_momentum=bn_momentum)\n",
    "\n",
    "        # Loop over the number of layers in the stage.\n",
    "        for j in range(stages[i] - 1):\n",
    "            x = residual_module(data=x,\n",
    "                                filters=filters[i + 1],\n",
    "                                stride=(1, 1),\n",
    "                                bn_eps=bn_eps,\n",
    "                                bn_momentum=bn_momentum)\n",
    "\n",
    "    x = BatchNormalization(axis=-1,\n",
    "                           epsilon=bn_eps,\n",
    "                           momentum=bn_momentum)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "    x = Softmax()(x)\n",
    "\n",
    "    return Model(inputs, x, name='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(image_path, target_size=(32,32)):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, np.float32)\n",
    "    image -= CINIC_MEAN_RGB\n",
    "    image = tf.image.resize(image, target_size)\n",
    "\n",
    "    label = tf.strings.split(image_path, os.path.sep)[-2]\n",
    "    label = (label == CINIC_10_CLASSES)\n",
    "    label = tf.dtypes.cast(label, tf.float32)\n",
    "\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_pattern, shuffle=False):\n",
    "    dataset = (tf.data.Dataset\n",
    "               .list_files(data_pattern)\n",
    "               .map(load_images_and_labels,\n",
    "                    num_parallel_calls=AUTOTUNE)\n",
    "                    .batch(BATCH_SIZE))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    return dataset.prefetch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "CINIC_MEAN_RGB = np.array([0.47889522, 0.47227842, 0.43047404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "CINIC_10_CLASSES = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&isAllowed=y'\n",
    "DATA_NAME = 'cinic10'\n",
    "FILE_EXTENSION = 'tar.gz'\n",
    "FILE_NAME = '.'.join([DATA_NAME, FILE_EXTENSION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_file_location = get_file(origin=DATASET_URL, fname=FILE_NAME, extract=False)\n",
    "\n",
    "data_directory, _ = downloaded_file_location.rsplit(os.path.sep, maxsplit=1)\n",
    "data_directory = os.path.sep.join([data_directory, DATA_NAME])\n",
    "tar = tarfile.open(downloaded_file_location)\n",
    "\n",
    "if not os.path.exists(data_directory):\n",
    "    tar.extractall(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pattern = os.path.sep.join([data_directory, 'train/*/*.png'])\n",
    "test_pattern = os.path.sep.join([data_directory, 'test/*/*.png'])\n",
    "valid_pattern = os.path.sep.join([data_directory, 'valid/*/*.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "train_dataset = prepare_dataset(train_pattern, shuffle=True)\n",
    "test_dataset = prepare_dataset(test_pattern, shuffle=True)\n",
    "valid_dataset = prepare_dataset(valid_pattern, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "residual_module() got an unexpected keyword argument 'stride'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\Computer-Vision\\2.Performing-Image-Classifier\\4.Implementing-ResNet-from-scratch.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m build_resnet(input_shape\u001b[39m=\u001b[39;49m(\u001b[39m32\u001b[39;49m,\u001b[39m32\u001b[39;49m,\u001b[39m3\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                      classes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                      stages\u001b[39m=\u001b[39;49m(\u001b[39m9\u001b[39;49m,\u001b[39m9\u001b[39;49m,\u001b[39m9\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                      filters\u001b[39m=\u001b[39;49m(\u001b[39m64\u001b[39;49m,\u001b[39m64\u001b[39;49m,\u001b[39m128\u001b[39;49m,\u001b[39m256\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                      reg\u001b[39m=\u001b[39;49m\u001b[39m5e-3\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m               optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model_chechpoint_callback \u001b[39m=\u001b[39m ModelChechpoint(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     filepath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./model\u001b[39m\u001b[39m{epoch:02d}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{val_accuracy:.2f}\u001b[39;00m\u001b[39m.hdf5\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     save_weights_onlu\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\hp\\Documents\\GitHub\\Computer-Vision\\2.Performing-Image-Classifier\\4.Implementing-ResNet-from-scratch.ipynb Cell 13\u001b[0m in \u001b[0;36mbuild_resnet\u001b[1;34m(input_shape, classes, stages, filters, reg, bn_eps, bn_momentum)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(stages)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# Initialize the stride, then apply a residual module\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# used to reduce the spatial size of the input volume.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     stride \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m residual_module(data\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                         filters\u001b[39m=\u001b[39;49mfilters[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                         reduce\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                         bn_eps\u001b[39m=\u001b[39;49mbn_eps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                         bn_momentum\u001b[39m=\u001b[39;49mbn_momentum)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# Loop over the number of layers in the stage.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Documents/GitHub/Computer-Vision/2.Performing-Image-Classifier/4.Implementing-ResNet-from-scratch.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(stages[i] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: residual_module() got an unexpected keyword argument 'stride'"
     ]
    }
   ],
   "source": [
    "model = build_resnet(input_shape=(32,32,3),\n",
    "                     classes=10,\n",
    "                     stages=(9,9,9),\n",
    "                     filters=(64,64,128,256),\n",
    "                     reg=5e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[\"accracy\"])\n",
    "\n",
    "model_chechpoint_callback = ModelChechpoint(\n",
    "    filepath='./model{epoch:02d}-{val_accuracy:.2f}.hdf5',\n",
    "    save_weights_onlu=False,\n",
    "    monitor='val_accuracy')\n",
    "\n",
    "EPOCHS=10\n",
    "model.fit(train_dataset,\n",
    "          validation_data=valid_dataset,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[model_chechpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('model.38-0.72hdf5')\n",
    "result = model.evaluate(test_datatset)\n",
    "print(f'Test accuracy: {result[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
